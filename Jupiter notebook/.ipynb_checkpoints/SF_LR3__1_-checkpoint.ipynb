{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.metrics import mean_squared_error, f1_score, accuracy_score, roc_curve, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Логистическая регрессия. Реализация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция ошибки для логистической регрессии в случае бинарной классификации называется бинарной кросс-энтропией и записывается следующим образом:\n",
    "$$L=-\\frac{1}{n}(y_i \\log h_{\\theta}(x_i) + (1-y_i) \\log(1-h_{\\theta}(x_i))),$$\n",
    "где $x_i$ — вектор признаков $i$-го примера из обучающей выборки, $y_i$ — истинный класс для соответствующего примера (0 или 1), $n$ — число примеров в обучающей выборке, $h_{\\theta}(x)$ — sigmoid функция, равная:\n",
    "$$h_{\\theta}(x)=\\frac{1}{1+\\exp^{-\\theta x}},$$\n",
    "где $\\theta$ — вектор параметров логистической регрессии, $x$ - вектор признаков объекта из выборки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соответствующий градиент функции ошибки равен:\n",
    "$$\\nabla L=\\frac{1}{n}\\sum_{i=1}^{n}{(h_{\\theta}(x_i)-y_i)x_i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализация логистической регрессии будет основана на оптимизации функции ошибки градиентным спуском."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве экспериментальных данных возьмем датасет о доходах граждан в различных странах [Adult Income](https://archive.ics.uci.edu/ml/datasets/Adult) и сделаем необходимую предобработку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File ./data/adult.data does not exist: './data/adult.data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-279c48ee5ecb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m                            \u001b[1;34m'education-num'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'marital-status'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'occupation'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                            \u001b[1;34m'relationship'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'race'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'sex'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'capital-gain'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m                            'capital-loss', 'hours-per-week', 'native-country', 'salary'])\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    674\u001b[0m         )\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 448\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1115\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1891\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1893\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File ./data/adult.data does not exist: './data/adult.data'"
     ]
    }
   ],
   "source": [
    "adult = pd.read_csv('./data/adult.data',\n",
    "                    names=['age', 'workclass', 'fnlwgt', 'education',\n",
    "                           'education-num', 'marital-status', 'occupation',\n",
    "                           'relationship', 'race', 'sex', 'capital-gain',\n",
    "                           'capital-loss', 'hours-per-week', 'native-country', 'salary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Описание датасета\n",
    "\n",
    "# with open('./data/adult.names', 'r') as f:\n",
    "#     names = f.read()\n",
    "# print(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Избавиться от лишних признаков\n",
    "adult.drop(['native-country'], axis=1, inplace=True)\n",
    "# Сконвертировать целевой столбец в бинарные значения\n",
    "adult['salary'] = (adult['salary'] != ' <=50K').astype('int32')\n",
    "# Сделать one-hot encoding для некоторых признаков\n",
    "adult = pd.get_dummies(adult, columns=['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Нормализовать нуждающиеся в этом признаки\n",
    "a_features = adult[['age', 'education-num', 'hours-per-week', 'fnlwgt', 'capital-gain', 'capital-loss']].values\n",
    "norm_features = (a_features - a_features.mean(axis=0)) / a_features.std(axis=0)\n",
    "adult.loc[:, ['age', 'education-num', 'hours-per-week', 'fnlwgt', 'capital-gain', 'capital-loss']] = norm_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разбить таблицу данных на матрицы X и y\n",
    "X = adult[list(set(adult.columns) - set(['salary']))].values\n",
    "y = adult['salary'].values\n",
    "\n",
    "# Добавить фиктивный столбец единиц (bias линейной модели)\n",
    "X = np.hstack([np.ones(X.shape[0])[:, np.newaxis], X])\n",
    "m = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Реализовать функцию sigmoid\n",
    "def sigmoid(X, theta):\n",
    "    return 1. / (1. + np.exp(-X.dot(theta)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Реализовать функцию, вычисляющую градиент бинарной кросс-энтропии\n",
    "def calc_binary_cross_entropy_grad(X, y, theta):\n",
    "    n = X.shape[0]\n",
    "    grad = 1. / n * X.transpose().dot(sigmoid(X, theta) - y)\n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_step(theta, theta_grad, alpha):\n",
    "    return theta - alpha * theta_grad\n",
    "def optimize(X, y, grad_func, start_theta, alpha, n_iters):\n",
    "    theta = start_theta.copy()\n",
    "    \n",
    "    for i in range(n_iters):\n",
    "        theta_grad = grad_func(X, y, theta)\n",
    "        theta = gradient_step(theta, theta_grad, alpha)\n",
    "    \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оптимизировать параметр линейной регрессии theta на всех данных\n",
    "theta = optimize(X, y, calc_binary_cross_entropy_grad, np.ones(m), 1., 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_logisitc_metrics(y_true, y_pred):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    print(f'acc = {acc:.2f} F1-score = {f1:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сделать предсказания на тренировочной выборке и\n",
    "# посчитать значение метрики accuracy и F1-score\n",
    "y_pred = sigmoid(X, theta) > 0.5\n",
    "print_logisitc_metrics(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разбить выборку на train/valid, оптимизировать theta,\n",
    "# сделать предсказания и посчитать ошибку F1-score\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2)\n",
    "theta = optimize(X_train, y_train, calc_binary_cross_entropy_grad, np.ones(m), 1., 300)\n",
    "y_pred = sigmoid(X_valid, theta) > 0.5\n",
    "\n",
    "print_logisitc_metrics(y_valid, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Отрисовать ROC кривую\n",
    "def calc_and_plot_roc(y_true, y_pred_proba):\n",
    "    # Посчитать значения ROC кривой и значение площади под кривой AUC\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pred_proba)\n",
    "    roc_auc = roc_auc_score(y_true, y_pred_proba)\n",
    "    \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.2f}')\n",
    "    plt.title('Receiver Operating Characteristic', fontsize=15)\n",
    "    plt.xlabel('False positive rate (FPR)', fontsize=15)\n",
    "    plt.ylabel('True positive rate (TPR)', fontsize=15)\n",
    "    plt.legend(fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вычислить вероятности принадлежности классу 1 для каждого объекта из валидационной выборки\n",
    "y_pred_proba = sigmoid(X_valid, theta)\n",
    "calc_and_plot_roc(y_valid, y_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Добавление регуляризации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Оборачивание линейной регрессии в класс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegOptimizer():\n",
    "    def __init__(self, alpha, n_iters):\n",
    "        self.theta = None\n",
    "        self._alpha = alpha\n",
    "        self._n_iters = n_iters\n",
    "    \n",
    "    def gradient_step(self, theta, theta_grad):\n",
    "        return theta - self._alpha * theta_grad\n",
    "    \n",
    "    def grad_func(self, X, y, theta):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def optimize(self, X, y, start_theta, n_iters):\n",
    "        theta = start_theta.copy()\n",
    "\n",
    "        for _ in range(n_iters):\n",
    "            theta_grad = self.grad_func(X, y, theta)\n",
    "            theta = self.gradient_step(theta, theta_grad)\n",
    "\n",
    "        return theta\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        m = X.shape[1]\n",
    "        start_theta = np.ones(m)\n",
    "        self.theta = self.optimize(X, y, start_theta, self._n_iters)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinReg(RegOptimizer):\n",
    "    def grad_func(self, X, y, theta):\n",
    "        n = X.shape[0]\n",
    "        grad = 1. / n * X.transpose().dot(X.dot(theta) - y)\n",
    "\n",
    "        return grad\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.theta is None:\n",
    "            raise Exception('You should train the model first')\n",
    "        \n",
    "        y_pred = X.dot(self.theta)\n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_regression_metrics(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    print(f'MSE = {mse:.2f}, RMSE = {rmse:.2f}')\n",
    "def prepare_boston_data():\n",
    "    data = load_boston()\n",
    "    X, y = data['data'], data['target']\n",
    "    # Нормализовать даннные с помощью стандартной нормализации\n",
    "    X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "    # Добавить фиктивный столбец единиц (bias линейной модели)\n",
    "    X = np.hstack([np.ones(X.shape[0])[:, np.newaxis], X])\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg = LinReg(0.01, 500)\n",
    "X, y = prepare_boston_data()\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg.fit(X_train, y_train)\n",
    "y_pred = linreg.predict(X_valid)\n",
    "print_regression_metrics(y_valid, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Оборачивание логистической регрессии в класс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogReg(RegOptimizer):\n",
    "    def sigmoid(self, X, theta):\n",
    "        return 1. / (1. + np.exp(-X.dot(theta)))\n",
    "    \n",
    "    def grad_func(self, X, y, theta):\n",
    "        n = X.shape[0]\n",
    "        grad = 1. / n * X.transpose().dot(self.sigmoid(X, theta) - y)\n",
    "\n",
    "        return grad\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        return self.sigmoid(X, self.theta)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.theta is None:\n",
    "            raise Exception('You should train the model first')\n",
    "        \n",
    "        y_pred = self.predict_proba(X) > 0.5\n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_adult_data():\n",
    "    adult = pd.read_csv('./data/adult.data',\n",
    "                        names=['age', 'workclass', 'fnlwgt', 'education',\n",
    "                               'education-num', 'marital-status', 'occupation',\n",
    "                               'relationship', 'race', 'sex', 'capital-gain',\n",
    "                               'capital-loss', 'hours-per-week', 'native-country', 'salary'])\n",
    "    \n",
    "    # Избавиться от лишних признаков\n",
    "    adult.drop(['native-country'], axis=1, inplace=True)\n",
    "    # Сконвертировать целевой столбец в бинарные значения\n",
    "    adult['salary'] = (adult['salary'] != ' <=50K').astype('int32')\n",
    "    # Сделать one-hot encoding для некоторых признаков\n",
    "    adult = pd.get_dummies(adult, columns=['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex'])\n",
    "    \n",
    "    # Нормализовать нуждающиеся в этом признаки\n",
    "    a_features = adult[['age', 'education-num', 'hours-per-week', 'fnlwgt', 'capital-gain', 'capital-loss']].values\n",
    "    norm_features = (a_features - a_features.mean(axis=0)) / a_features.std(axis=0)\n",
    "    adult.loc[:, ['age', 'education-num', 'hours-per-week', 'fnlwgt', 'capital-gain', 'capital-loss']] = norm_features\n",
    "    \n",
    "    # Разбить таблицу данных на матрицы X и y\n",
    "    X = adult[list(set(adult.columns) - set(['salary']))].values\n",
    "    y = adult['salary'].values\n",
    "\n",
    "    # Добавить фиктивный столбец единиц (bias линейной модели)\n",
    "    X = np.hstack([np.ones(X.shape[0])[:, np.newaxis], X])\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogReg(1., 300)\n",
    "X, y = prepare_adult_data()\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разбить выборку на train/valid, оптимизировать theta,\n",
    "# сделать предсказания и посчитать ошибку F1-score\n",
    "\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_valid)\n",
    "\n",
    "print_logisitc_metrics(y_valid, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = logreg.predict_proba(X_valid)\n",
    "calc_and_plot_roc(y_valid, y_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В случаях линейной и логистической регрессии будем добавлять к функции ошибки регуляризующую часть как:\n",
    "$$\\frac{\\lambda}{2m}\\sum_{j}^{m}{\\theta_j^2},$$\n",
    "где $\\theta$ — вектор параметров линейной модели без фиктивного признака (intercept/bias term), $m$ — количество нефиктивных признаков, $\\lambda$ — параметр регуляризации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Добавление регуляризатора в линейную регрессию"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После добавления регуляризации функция ошибки линейной регрессии будет выглядеть следующим образом:\n",
    "$$L=\\frac{1}{2n} * \\sum_{i=1}^{n}{(y_i - \\theta^Tx_i)^2} + \\frac{\\lambda}{2m}\\sum_{j}^{m}{\\theta_j^2}$$\n",
    "А ее градиент по параметру $\\theta$:\n",
    "$$\\nabla L = \\frac{1}{n}\\sum_{i=1}^{n}{(\\theta^Tx_i - y_i) \\cdot x_i} + \\frac{\\lambda}{m}\\sum_{j=1}^{m}{\\theta_j} = \\frac{1}{n}X^T(X\\theta - y) + \\frac{\\lambda}{m}\\sum_{j=1}^{m}{\\theta_j}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinRegRegularized(LinReg):\n",
    "    def __init__(self, alpha, lambd, n_iters):\n",
    "        super(LinRegRegularized, self).__init__(alpha, n_iters)\n",
    "        self._lambd = lambd\n",
    "    \n",
    "    def grad_func(self, X, y, theta):\n",
    "        n = X.shape[0]\n",
    "        grad = 1. / n * X.transpose().dot(X.dot(theta) - y)\n",
    "        grad_term = self._lambd * np.mean(theta)\n",
    "\n",
    "        return grad + grad_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg = LinRegRegularized(alpha=0.01, lambd=0.05, n_iters=500)\n",
    "X, y = prepare_boston_data()\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg.fit(X_train, y_train)\n",
    "y_pred = linreg.predict(X_valid)\n",
    "print_regression_metrics(y_valid, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Добавление регуляризатора в логистическую регрессию"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция ошибки для логистической регрессии в случае бинарной классификации с регуляризатором записывается следующим образом:\n",
    "$$L=-\\frac{1}{n}(y_i \\log h_{\\theta}(x_i) + (1-y_i) \\log(1-h_{\\theta}(x_i)))+\\frac{\\lambda}{2m}\\sum_{j}^{m}{\\theta_j^2},$$\n",
    "где $x_i$ — вектор признаков $i$-го примера из обучающей выборки, $y_i$ — истинный класс для соответствующего примера (0 или 1), $n$ — число примеров в обучающей выборке, $m$ — количество нефиктивных признаков, $\\lambda$ — параметр регуляризации, $h_{\\theta}(x)$ — sigmoid функция, равная:\n",
    "$$h_{\\theta}(x)=\\frac{1}{1+\\exp^{-\\theta x}},$$\n",
    "где $\\theta$ — вектор параметров логистической регрессии, $x$ - вектор признаков объекта из выборки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соответствующий градиент функции ошибки равен:\n",
    "$$\\nabla L=\\frac{1}{n}\\sum_{i=1}^{n}{(h_{\\theta}(x_i)-y_i)x_i}+\\frac{\\lambda}{m}\\sum_{j}^{m}{\\theta_j}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogRegRegularized(LogReg):\n",
    "    def __init__(self, alpha, lambd, n_iters):\n",
    "        super(LogRegRegularized, self).__init__(alpha, n_iters)\n",
    "        self._lambd = lambd\n",
    "    \n",
    "    def grad_func(self, X, y, theta):\n",
    "        n = X.shape[0]\n",
    "        grad = 1. / n * X.transpose().dot(self.sigmoid(X, theta) - y)\n",
    "        grad_term = self._lambd * np.mean(theta)\n",
    "\n",
    "        return grad + grad_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogRegRegularized(alpha=1., lambd=1., n_iters=300)\n",
    "X, y = prepare_adult_data()\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разбить выборку на train/valid, оптимизировать theta,\n",
    "# сделать предсказания и посчитать ошибку F1-score\n",
    "\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_valid)\n",
    "\n",
    "print_logisitc_metrics(y_valid, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_pred_proba = logreg.predict_proba(X_valid)\n",
    "calc_and_plot_roc(y_valid, y_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
